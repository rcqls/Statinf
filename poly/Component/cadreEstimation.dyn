

Dans ce cadre, tout probl{\`e}me pratique doit se ramener {\`a} l'{\'e}tude d'une unique variable d'int{\'e}r{\^e}t not{\'e}e ici $Y$ (pouvant aussi {\^e}tre vue comme une future unique donn{\'e}e).
En pratique, nous disposerons d'un jeu de $n$ donn{\'e}es $\Vect{y}=(y_1,\ldots,y_n)$ (i.e. un ``paquet'' de $n$ observations ``ind{\'e}pendantes'' de $Y$) qui peut par cons{\'e}quent {\^e}tre vu comme un r{\'e}sultat possible d'un futur jeu de $n$ donn{\'e}es $\Vect{Y}=(Y_1,\ldots,Y_n)$. Afin d'expliciter le param{\`e}tre d'int{\'e}r{\^e}t intimement li{\'e} dans les probl{\'e}matiques du cours {\`a} la variable d'int{\'e}r{\^e}t $Y$, nous imaginerons disposer d'une infinit{\'e} de donn{\'e}es virtuelles $y_{[1]},\ldots,y_{[m]},\ldots$ dont la notation en indice entre crochet (i.e. ``$_{[\cdot]}$'') nous rappelle qu'il ne faut pas les confondre avec le jeu des $n$ donn{\'e}es $y_1,\ldots,y_n$ qui seront bien r{\'e}elles. Rappelons aussi que dans ce cours les tailles $n$ des donn{\'e}es \textbf{r{\'e}elles} et $m$ des donn{\'e}es \textbf{virtuelles} ont a priori des ordres de grandeur compl{\`e}tement diff{\'e}rents, {\`a} savoir $n$ plut{\^o}t raisonnablement grand  et $m$ aussi grand que possible voire infini.

Les param{\`e}tres d'int{\'e}r{\^e}t {\'e}tudi{\'e}s seront soit des moyennes soit des variances.
\subsubsection{Moyenne}
 La moyenne not{\'e}e $\mu_Y$ ou plus simplement $\mu$ (plut{\^o}t appel{\'e}e esp{\'e}rance de $Y$ dans l'\textbf{Approche Classique des Probabilit{\'e}s} et not{\'e}e $\mathbb{E}(Y)$) s'exprime via l'\textbf{Approche Exp{\'e}rimentale des Probabilit{\'e}s} par~:\\
 \centerline{\fbox{$\mu_Y=\displaystyle{\lim_{m\to+\infty} \overline{y}_{[m]}}$} o{\`u} \fbox{$\overline{y}_{[m]}=\displaystyle{\frac 1m\sum_{i=1}^my_{[i]}}$}}
Soulignons toutefois que si les donn{\'e}es sont exclusivement {\`a} valeurs 0 ou 1, la moyenne devient une proportion (ou probabilit{\'e}) et sera not{\'e}e $p$ plut{\^o}t que $\mu$.

Rappelons qu'une future estimation $\Est{\mu_Y}{Y}$ de $\mu_Y$ est tout simplement $\overline{Y}_n=\displaystyle{\frac 1n\sum_{i=1}^nY_i}$ (not{\'e}e aussi $\overline{Y}$). 
\subsubsection{Variance}
La variance not{\'e}e $\sigma^2_Y$ ou plus simplement $\sigma^2$ (conservant la m{\^e}me d{\'e}nomination dans l'\textbf{Approche Classique des Probabilit{\'e}s} et not{\'e}e $\Var(Y)$) s'exprime via l'\textbf{Approche Exp{\'e}rimentale des Probabilit{\'e}s} par~:\\ 
\centerline{\fbox{$\sigma^2_Y=\displaystyle{\lim_{m\to+\infty}\frac 1m\sum_{i=1}^m(y_{[i]}- \overline{y}_{[m]})^2}$}}
         
Dans le cadre de grands {\'e}chantillons (voir plus loin), il est plus qu'int{\'e}ressant de  noter que la variance est aussi une moyenne. En effet, nous pouvons {\'e}crire 
$\sigma^2_Y=\mu_{\ddot{Y}}$ puisque $\Var(Y)=\mathbb{E}((Y-\mu_Y)^2)=\mathbb{E}(\ddot{Y}) $ o{\`u} $\ddot{Y}=(Y-\mu_Y)^2$ est le carr{\'e} de la variable al{\'e}atoire $Y$ pr{\'e}alablement centr{\'e}e. 
Le vecteur des futures donn{\'e}es  $((Y_1-\mu_Y)^2,\ldots,(Y_n-\mu_Y)^2)$ {\'e}tant inaccessible puisque $\mu_Y$ est inconnu, nous le remplacerons par $\Vect{\ddot{Y}}=((Y_1-\overline{Y})^2,\ldots,(Y_n-\overline{Y})^2 )$. Ainsi, nous pourrions aussi proposer $\Est{\mu_{\ddot{Y}}}{\ddot{Y}}$ comme future estimation de $\sigma^2_Y=\mu_{\ddot{Y}}$ (plut{\^o}t lorsque la taille $n$ des donn{\'e}es sera suffisamment grande).\\

\subsection{Cadre d'estimation {\`a} deux {\'e}chantillons (ind{\'e}pendants)}
ll y a dans ce cadre deux variables d'int{\'e}r{\^e}ts $Y^{(1)}$ et $Y^{(2)}$ (``ind{\'e}pendantes'') dont on cherche soit {\`a} comparer les moyennes soit les variances {\`a} partir de deux {\'e}chantillons, l'un $\mathbf{y}^{(1)}=(y_1^{(1)},\ldots,y_{n^{(1)}}^{(1)})$ de taille $n^{(1)}$ et  l'autre $\mathbf{y}^{(2)}=(y_1^{(2)},\ldots,y_{n^{(2)}}^{(2)})$ de taille $n^{(2)}$. Il en d{\'e}coule deux futurs jeux de donn{\'e}es  $\mathbf{Y}^{(1)}=(Y_1^{(1)},\ldots,Y_{n^{(1)}}^{(1)})$ et $\mathbf{Y}^{(2)}=(Y_1^{(2)},\ldots,Y_{n^{(2)}}^{(2)})$. Pour homog{\'e}n{\'e}iser ce cas avec celui {\`a} un seul {\'e}chantillon, nous noterons $\mathbf{Y}$, le vecteur aggr{\'e}g{\'e} de toutes les futures donn{\'e}es $\mathbf{Y}=(\mathbf{Y}^{(1)},\mathbf{Y}^{(2)})$ de taille $n=n^{(1)}+n^{(2)}$.
De mani{\`e}re analogue au cas d'un seul {\'e}chantillon, nous imaginons disposer de deux infinit{\'e}s de donn{\'e}es virtuelles, l'une $y_{[1]}^{(1)},\ldots,y_{[m]}^{(1)},\ldots$ relative {\`a} $Y^{(1)}$ et l'autre $y_{[1]}^{(2)},\ldots,y_{[m]}^{(2)},\ldots$ relative {\`a} $Y^{(2)}$. Pour $j=1$ ou $j=2$, on peut alors exprimer~:
\begin{itemize}
  \item la \textbf{moyenne} $\mu_{Y^{(j)}}$ ou plus simplement $\mu^{(j)}$ par  \fbox{$\mu^{(j)}=\displaystyle{\lim_{m\to+\infty} \overline{y}_{[m]}^{(j)}}$} o{\`u} \fbox{$\overline{y}_{[m]}^{(j)}=\displaystyle{\frac 1m\sum_{i=1}^my_{[i]}^{(j)}}$}
  \item la \textbf{variance} $\sigma^2_{Y^{(j)}}$ ou plus simplement $\sigma_{(j)}^2$ d{\'e}finie par \fbox{$\sigma_{(j)}^2=\displaystyle{\lim_{m\to+\infty}\frac 1m\sum_{i=1}^m(y_{[i]}^{(j)}- \overline{y}_{[m]}^{(j)})^2}$}  
\end{itemize}

Nous sommes alors en mesure d'introduire les param{\`e}tres servant {\`a} comparer respectivement les moyennes et les variances.
\begin{itemize}
  \item \textbf{comparaison de moyennes} s'{\'e}tudiant soit {\`a} partir de la \textbf{diff{\'e}rence de moyennes} $d_\mu=\mu^{(1)}-\mu^{(2)}$ soit {\`a} partir du \textbf{rapport de moyennes} $r_\mu=\mu^{(1)}/\mu^{(2)}$.
  \item \textbf{comparaison de variances} s'{\'e}tudiant soit {\`a} partir de la \textbf{diff{\'e}rence de variances} $d_{\sigma^2}=\sigma_{(1)}^2-\sigma_{(2)}^2$ soit {\`a} partir du \textbf{rapport de variances} $ r_{\sigma^2}=\sigma_{(1)}^2/\sigma_{(2)}^2$.
\end{itemize} 
Insistons sur le fait que les utilisations d'une diff{\'e}rence ou d'un rapport ne sont pas anodines puisqu'elles permettent de traiter des assertions d'int{\'e}r{\^e}t diff{\'e}rentes.
